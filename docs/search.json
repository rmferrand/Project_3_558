[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA(Exploratory Data Analysis)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cowplot)\nlibrary(scales)\ndiabetes &lt;- read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA(Exploratory Data Analysis)",
    "section": "Introduction",
    "text": "Introduction\nHello! Welcome to My (Robbie’s) Final Project for ST558. The purpose of this project is to explore a diabetes dataset, from Kaggle. The dataset also has a codebook, which can be found here.\nThis page takes the user through exploratory data analysis of the diabetes dataset, including data validation, distribution analysis, and determining an overall objective for the Modeling Page.\n\nVariables Used in this Project\nAll of the variables can be found at the links above. Throughout the project, the creator decided to change objectives a few times, but the main objective was investigating relevant categorical variables of interest. Note that the creator has decided to exclude the BMI variable, which is the only numerical variable available.\n\nResponse variable:\n\nDiabetes_binary: 0 if no diabetes, 1 if diabetes\n\nThis is our variable of interest. We wish to see if we can predict if an individual has diabetes based on predictor variables.\n\n\nPredictor variables:\n\nHighBP(0,1): 0 if individual does not have high blood pressure, 1 if individual does have high blood pressure\nHighChol(0,1): 0 if individual does not have high cholesterol, 1 if individual does have high cholesterol\nFruits(0,1): 0 if individual does not consume fruit daily, 1 if individual does consume fruit daily\nDiffWalk(0,1): 0 if individual does not have trouble walking, 1 if individual does have difficulty walking\nSmoker: 0 if individual has not smoked 5 packs of cigarettes (100 cigarettes) in their lifetime, 1 if individual has smoked 5 packs of cigarettes (100 cigarettes) in their lifetime\nCholCheck(0,1): 0 if individual has not had their cholesterol levels checked in the last 5 years, 1 if individual has had their cholesterol levels checked in the last 5 years\nHeartDiseaseorAttack(0,1): 0 if individual does not have heart disease AND has not had a heart attack, 1 if individual has had either one or both\nHvyAlcoholConsump(0,1): 0 if individual is not a heavy drinker, 1 if individual is a heavy drinker (&gt;14 drinks men, &gt;7 women)\nSex(0,1): 0 if individual is female, 1 if individual is male\nAge(1-13): This variable captures if a user is in one of the following age categories:\n1: 18-24, 2: 25-29, 3: 30-34, 4: 35-39, 5: 40-44, 6: 45-49, 7: 50-54,\n8: 55-59, 9: 60-64, 10: 65-69, 11: 70-75, 12: 75-59, 13: 80-99\nGenHlth(1-5): This variable captures a user’s overall health:\n1: Excellent 2: Very Good 3: Good 4: Fair 5: Poor\n\nThe following variables were modified by the creator to accentuate differences in distribution.\n\nMentHlth(0-2): scale of days with bad mental health.\n0: Individual experiences no mental health pain.\n1: Individual experiences some mental health pain in the last 30 days.\n2: Individual experiences mental health pain every day.\nPhysHlth(0-2): scale of days with bad physical health.\n0: Individual experiences no physical health pain.\n1: Individual experiences some physical health pain in the last 30 days.\n2: Individual experiences physical health pain every day."
  },
  {
    "objectID": "EDA.html#exploratory-data-analysis",
    "href": "EDA.html#exploratory-data-analysis",
    "title": "EDA(Exploratory Data Analysis)",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nMy general thought process for EDA is to first investigate if there are any missing values first. If so, then investigate those rows/columns. Second, I want to check the structures of the data and make sure they all make sense. Lastly, let’s analyze some distributions of the individual categorical variables, and see if we can notice any cool trends. This will help us find an objective, and help us narrow down to some variables that we want to look at.\n\nMissing Values\n\n(sum(is.na(diabetes)))\n\n[1] 0\n\n\nGreat! It looks like there are none. I think a natural next step would be to investigate all of the variables, and make sure their names are legible and their values are logical.\n\n\nVariable Names\n\nas_tibble(diabetes, width = Inf)\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nBased on the given variable descriptions above, I believe that the variable names are rather sensical. However, it looks like the data was read in as entirely double and not as factors.\n\n\nChanging Data to Factors\n\nBinary\nFirst, let’s start with all the variables that are binary. We can give Sex labels for ease of use, since the rest of them are just yes/no.\n\ndiabetes &lt;- diabetes |&gt;\n  mutate(across(c(1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18), as.factor)) |&gt;\n  mutate(Sex = factor(Sex, levels = c(0,1), labels = c(\"Female\", \"Male\")))\nstr(diabetes)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : num  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : num  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num  3 1 8 6 4 8 7 4 1 3 ...\n\n\n\n\nPhysHlth MentHlth\nPhysHlth and MentHlth are very interesting variables. A quick peep into the future showed me their distributions:\n\n#shoutout cowplot\ngg_phys &lt;- ggplot(diabetes, aes(x = PhysHlth)) +\n  geom_histogram(bins = 10, fill = \"salmon\", color = \"black\") +\n  labs(title = \"Physical Pain over Last 30 Days\", x = \"Days\", y = \"Frequency\") +\n  theme_minimal()\n\ngg_ment &lt;- ggplot(diabetes, aes(x = MentHlth)) +\n  geom_histogram(bins = 10, fill = \"cyan\", color = \"black\") +\n  labs(title = \"Mental Pain over Last 30 Days\", x = \"Days\", y = \"Frequency\") +\n  theme_minimal()\n\nplot_grid(gg_phys, gg_ment)\n\n\n\n\nThere are a few considerations that come to mind. Treating this as a numerical variable possibly loses information between 0 days and 1 day, since someone who is having no mental/physical pain is surely different than someone who is experiencing any sort of pain whatsoever. Furthermore, there is also a jump of individuals who are having pain all the time, which seems like another dichotic, important category. That is why I’ve decided to transform the variables into a 0,1,2 categorical ordered variable.\n\n\nOrdered Factors\nNext, We have other factors with multiple levels that need to be named and ordered. We will treat Age, GenHlth,Education, and Income as ordered factors since it follows the same trend as 1&lt;2&lt;…, etc.\n\ndiabetes &lt;- diabetes |&gt; \n  mutate(GenHlth = factor(GenHlth, levels = c(1, 2, 3, 4, 5), labels = c(\"Excellent\", \"Very Good\", \"Good\", \"Fair\", \"Poor\"), ordered = TRUE)) |&gt;\n  mutate(MentHlth = case_when(\n    MentHlth == 0 ~ 0,\n    MentHlth &gt;= 1 & MentHlth &lt;= 29 ~ 1,\n    MentHlth == 30 ~ 2\n  )) |&gt;\n  mutate(MentHlth = factor(MentHlth,levels = c(0, 1, 2), labels = c(\"No Pain\", \"Some Pain\", \"Everyday Pain\"),ordered = TRUE)) |&gt;\nmutate(PhysHlth = case_when(\n    PhysHlth == 0 ~ 0,\n    PhysHlth &gt;= 1 & PhysHlth &lt;= 29 ~ 1,\n    PhysHlth == 30 ~ 2\n  )) |&gt;\n  mutate(PhysHlth = factor(PhysHlth,levels = c(0, 1, 2), labels = c(\"No Pain\", \"Some Pain\", \"Everyday Pain\"),ordered = TRUE)) |&gt;\n  mutate(Age = factor(Age,levels = c(1:13), labels = c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80-99\"),ordered = TRUE)) |&gt;\n  mutate(Education = factor(Education,levels = c(1:6), labels = c(\"NoSchool\",\"Elementary\",\"Some high school\",\"High school graduate\",\"Some college or technical school\",\"College graduate\"),ordered = TRUE)) |&gt;\n  mutate(Income = factor(Income,levels = c(1:8), labels = c(\"&lt;$10,000\",\"$10,000-$14,999\",\"$15,000-$19,999\",\"$20,000-$24,999\",\"$25,000 - $34,999\",\"$35,000 - $49,999\",\"$50,000 - $74,999\",\"&gt;=$75,000\"),ordered = TRUE))\n\n\n\nFinal Checks\nNow, ALL of the data should be correctly formatted and validated!\n\nstr(diabetes)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Ord.factor w/ 5 levels \"Excellent\"&lt;\"Very Good\"&lt;..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : Ord.factor w/ 3 levels \"No Pain\"&lt;\"Some Pain\"&lt;..: 2 1 3 1 2 1 1 1 3 1 ...\n $ PhysHlth            : Ord.factor w/ 3 levels \"No Pain\"&lt;\"Some Pain\"&lt;..: 2 1 3 1 1 2 2 1 3 1 ...\n $ DiffWalk            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Ord.factor w/ 13 levels \"18-24\"&lt;\"25-29\"&lt;..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Ord.factor w/ 6 levels \"NoSchool\"&lt;\"Elementary\"&lt;..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Ord.factor w/ 8 levels \"&lt;$10,000\"&lt;\"$10,000-$14,999\"&lt;..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\nLooks good!\n\n\n\nDetermining an Objective\n\nAttempt 1:\nNow that all of the data is correctly validated, we can determine which variables we are interested in looking at. I tend to like looking at more unexpected, or indirect routes to find relationships I wouldn’t obviously expect. I believe that diet and the byproducts of a poor one will lead to a higher probability of diabetes. I’ll approach this from a socioeconomic standpoint instead, plus a few extra variables that I am curious about.\nVariables of interest: AnyHealthCare, NoDocbcCost, GenHlth, MentHlth, PhysHlth, Sex, Age, Education, Income\n\ndiabetes_final &lt;- diabetes |&gt;\n  select(c(Diabetes_binary, AnyHealthcare, NoDocbcCost, GenHlth, MentHlth, PhysHlth, Sex, Age, Education, Income))\n\nReconsideration:\nHello! This is Robbie from the future after conducting Modeling. The models were all terrible, and the false negative rate was like 99%! So I’ve decided to include some extra predictors that will hopefully help us out. I understand failure is part of the process, and maybe I should’ve stuck with it, but I want a worthwhile model!\n\n\nFinal Objective\nThere are a few scattered things I’d like to explore. I am only particularly interested in categorical variables, so bye bye BMI. Blood pressure and Cholesterol variables are natural choices. I included demographic variables(Sex,Age) to see if they had any worthwhile contributions alongside the health variables. I also wanted to see how my newly transformed MentHlth and PhysHlth variables were going to do. I also included random variables, like Fruits, DiffWalk, HvyAlcoholConsump to see if there was any importance from those whatsoever, because I am naive to their relationship with diabetes. The other variables included seemed like natural choices. This removes a total of 8 predictors from the original dataset, just to explore some of these particular relationships I am interested in.\n\ndiabetes_final &lt;- diabetes |&gt;\n  select(c(Diabetes_binary, HighBP, HighChol, Fruits, DiffWalk, Smoker,CholCheck,HeartDiseaseorAttack, HvyAlcoholConsump, GenHlth, MentHlth, PhysHlth, Sex, Age))\n\n\n\n\nDistributions and Frequency\nNow that I’ve created a subset of predictors just based on interest (and to save me time!) Let’s go ahead and analyze our predictors and determine if any others need to be changed or taken out. MentHlth and PhysHlth were modified “post-hoc” in a sense, since I had moved them up and transformed them there.\n\nResponse Variable\n\nggplot(diabetes_final, aes(x = factor(Diabetes_binary), fill = factor(Diabetes_binary))) +\n  geom_bar(color = \"black\") +\n  labs(\n    title = \"Prevelance of Diabetes\",\n    x = \"Diabetes (0 = No, 1 = Yes)\",\n    y = \"Frequency\",\n    fill = \"Diabetes_binary\"\n  ) +\n  scale_fill_manual(values = c(\"0\" = \"darkblue\", \"1\" = \"yellow\")) +\n  theme_minimal() +\n  scale_y_continuous(labels = comma) +\n  theme(axis.text.x = element_text(size = 12))\n\n\n\n\nWe notice here that it is much more likely for a random individual in this dataset to not have diabetes than to have diabetes. Generally, we then would expect our predictor variables that have an extreme dichotic nature from person to person to do the best.\n\n\nBinary Variables\nLet’s create an ordered stacked bar plot so that we can look at the categorical variables better. Sure, we can make contingency tables, but I personally feel like this is nicer:\n\nbinary_data_hist &lt;- diabetes_final |&gt;\n  select(c(HvyAlcoholConsump, HeartDiseaseorAttack, DiffWalk, HighChol, HighBP, Smoker, Fruits, CholCheck)) |&gt;\n  pivot_longer(\n    cols = everything(), \n    names_to = \"Variable\", \n    values_to = \"Response\"\n  ) |&gt;\n  group_by(Variable, Response) |&gt;\n  summarise(Count = n(), .groups = \"drop\") |&gt;\n  group_by(Variable) |&gt;\n  mutate(Proportion = Count / sum(Count)) |&gt;\n  mutate(Variable = factor(Variable, levels = c(\n    \"HvyAlcoholConsump\", \"HeartDiseaseorAttack\", \"DiffWalk\", \n    \"HighChol\", \"HighBP\", \"Smoker\", \"Fruits\", \"CholCheck\"\n  )))\n\nggplot(binary_data_hist, aes(x = Variable, y = Proportion, fill = factor(Response))) +\n  geom_bar(stat = \"identity\", position = \"fill\", color = \"black\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"Binary Predictor Frequencies\",\n    x = \"Binary Variables(0 = No, 1 = Yes)\",\n    y = \"Proportion\",\n    fill = \"Response\"\n  ) +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1,size =12)) \n\n\n\n\nviola! So, we notice that not a lot of people are heavy drinkers, even less so than those who have heart attacks. Interesting! The good thing that we see here is that there is a lot of dispersion, some variables tend to have a lot of no’s, some are pretty equal, and CholCheck is very very high, with fruits also above the 50% margin.\n\nggplot(diabetes_final, aes(x = factor(Sex), fill = factor(Sex))) +\n  geom_bar(color = \"black\") +\n  labs(\n    title = \"Female/Male\",\n    x = \"Gender\",\n    y = \"Frequency\",\n    fill = \"Sex\"\n  ) +\n  scale_fill_manual(values = c(\"Female\" = \"hotpink\", \"Male\" = \"blue\")) +\n  scale_y_continuous(labels = comma) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 12))\n\n\n\n\nLooking at Sex, we see that there are slightly more females in the dataset than males. I don’t imagine this will cause any issues since the distributions are relatively similar, and close to the real world population as well.\n\n\nOrdered Predictors\nWith these predictors, I figure it may be helpful to look at them individually.\n\nggplot(diabetes_final, aes(x = factor(GenHlth), fill = factor(GenHlth))) +\n  geom_bar(color = \"black\") +\n  labs(\n    title = \"General Health\",\n    x = \"Health Rating\",\n    y = \"Frequency\",\n    fill = \"GenHlth\"\n  ) +\n  scale_fill_manual(values = c(\"Excellent\" = \"#FFB3BA\", \"Very Good\" = \"#FF6666\", \"Good\" = \"#FF0000\", \"Fair\" = \"#CC0000\", \"Poor\" = \"#990000\")) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 12))\n\n\n\n\nIn this case, we notice most individuals stick somewhere around the middle. Our natural intuition tells us that those towards the left of the graph (those in better health) are less likely to have diabetes.\n\nggplot(diabetes_final, aes(x = factor(MentHlth), fill = factor(MentHlth))) +\n  geom_bar(color = \"black\") +\n  labs(\n    title = \"Overall Mental Pain (Last 30 Days)\",\n    x = \"Occurence of Pain\",\n    y = \"Frequency\",\n    fill = \"MentHlth\"\n  ) +\n  scale_fill_manual(values = c(\"No Pain\" = \"#A8E6A1\", \"Some Pain\" = \"#56B956\", \"Everyday Pain\" = \"#006400\")) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 12))\n\n\n\n\n\nggplot(diabetes_final, aes(x = factor(PhysHlth), fill = factor(PhysHlth))) +\n  geom_bar(color = \"black\") +\n  labs(\n    title = \"Overall Physical Pain (Last 30 Days)\",\n    x = \"Occurence of Pain\",\n    y = \"Frequency\",\n    fill = \"PhysHlth\"\n  ) +\n  scale_fill_manual(values = c(\"No Pain\" = \"#A2DFF7\", \"Some Pain\" = \"#4682B4\", \"Everyday Pain\" = \"#000080\")) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 12))\n\n\n\n\nI like these graphs a lot better than the ones with 30 levels. We notice here that most individuals do not report any pain whatsoever, however there are a proportion of individuals who do report pain and I imagine that will be a pretty good predictor of diabetes rate.\nOut of curiosity, let’s look at a contingency table for the two of these variables:\n\nment_phys_table&lt;- table(diabetes_final$MentHlth, diabetes_final$PhysHlth)\nrownames(ment_phys_table) &lt;- c(\"No Mental Pain\", \"Some Mental Pain\", \"Everyday Mental Pain\")\ncolnames(ment_phys_table) &lt;- c(\"No Physical Pain\", \"Some Physical Pain\", \"Everyday Physical Pain\")\nment_phys_table\n\n                      \n                       No Physical Pain Some Physical Pain\n  No Mental Pain                 125985              40566\n  Some Mental Pain                30784              29501\n  Everyday Mental Pain             3283               4161\n                      \n                       Everyday Physical Pain\n  No Mental Pain                         9129\n  Some Mental Pain                       5627\n  Everyday Mental Pain                   4644\n\n\nThis displays some interesting relationships between mental and physical pain! It’s quite interesting to see that it’s not always that those who are experiencing everyday physical pain are also experiencing everyday mental pain.\nThis ends the EDA! Lastly, we will be saving our final diabetes file to an R object for use in EDA as well as the API.r.\n\nsaveRDS(diabetes_final, file = \"diabetes_final.rds\")\n\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Welcome to the modeling section! This is a direct continuation from the Exploratory Data Analysis, or (EDA) section found here.\nThe purpose of this section is to take the newly subsetted Diabetes dataset and compare it to two different models: the classification tree and the random forest. Based on the mean log loss (lower is better), we will determine which model is the best and use it for the API, which can be accessed upon request. All modeling steps and relevant intermediary notes will be annotated and explained."
  },
  {
    "objectID": "Modeling.html#train-and-test-split",
    "href": "Modeling.html#train-and-test-split",
    "title": "Modeling",
    "section": "Train and Test Split",
    "text": "Train and Test Split\nThe first step of any modeling journey is to create a test/train split. This is so that we can reduce overfitting - or essentially “hide” parts of the data to make sure we perform as well on the train set as we do on the test set. Let’s load in our necessary libraries and set our seed. Using the readRDS, we can access the previously created file from the EDA section.\n\nlibrary(tidymodels)\nset.seed(558)\n\ndiabetes_final &lt;- readRDS(file = \"diabetes_final.rds\")\ndiabetes_final &lt;- diabetes_final |&gt; slice_sample(n = 5000)\n\n\ndata_split &lt;- initial_split(diabetes_final, prop = 0.70)\ntrain_diabetes&lt;- training(data_split)\ntest_diabetes &lt;- testing(data_split)\n\n\ncv_folds &lt;- vfold_cv(train_diabetes, v = 5)\n\nWe will be using 5-fold cross validation, which will test and train the model on different iterations of the data."
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling",
    "section": "Classification Tree",
    "text": "Classification Tree\nThe classification tree splits up predictor spaces into different regions, such that we can make different predictions based on the particular “region” of the data we are exploring. Using\n\nRecipe\nWe first set up the recipe that we will be using for both the classification tree and the random forest. We have no numerical variables, so we are just concerned with making sure all of the categorical variables are “dummied”, or normalized such that we can correctly standardize them and their importance in the model.\n\ntree_rec &lt;- recipe(Diabetes_binary ~ ., data = train_diabetes) |&gt;\n  step_dummy(HighBP, HighChol, Fruits, DiffWalk, Smoker,CholCheck,HeartDiseaseorAttack, HvyAlcoholConsump, GenHlth, MentHlth, PhysHlth, Sex, Age)\n\n\n\nParamaterization of the Tree\nUsing the decision_tree() function, we can set the minimum number of data that are required for the node to be split, or for the tree to create a branch. We have set it low here, such that we can capture important trends within the data. When I was troubleshooting this step, I originally had it set to 500, but realized it was missing some trends in the data that improved prediction a lot.\nWhile setting up the grid using grid_regular() , I allowed for 10 levels which is generally considered a lot, but I figured it was important to create a large set of possible trees for the model to split on.\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 10,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\ntree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), levels = 10)\ntree_grid\n\n# A tibble: 100 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1    0.0000000001          1\n 2    0.000000001           1\n 3    0.00000001            1\n 4    0.0000001             1\n 5    0.000001              1\n 6    0.00001               1\n 7    0.0001                1\n 8    0.001                 1\n 9    0.01                  1\n10    0.1                   1\n# ℹ 90 more rows\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_mod)\n\n\n\nFitting the Tree\nNow, we can apply the workflow and grid we just created to find the best tree. We are interested in mean log loss, so we use that as the metric when applying fits.\n\ntree_fits &lt;- tree_wkf |&gt; \n  tune_grid(resamples = cv_folds,\n            grid = tree_grid,metrics = metric_set(mn_log_loss))\n\n#tree_fits |&gt; collect_metrics()\n\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          5 mn_log_loss binary     0.370     5  0.0152 Prepro…\n 2    0.000000001           5 mn_log_loss binary     0.370     5  0.0152 Prepro…\n 3    0.00000001            5 mn_log_loss binary     0.370     5  0.0152 Prepro…\n 4    0.0000001             5 mn_log_loss binary     0.370     5  0.0152 Prepro…\n 5    0.000001              5 mn_log_loss binary     0.370     5  0.0152 Prepro…\n 6    0.00001               5 mn_log_loss binary     0.370     5  0.0152 Prepro…\n 7    0.0001                5 mn_log_loss binary     0.370     5  0.0152 Prepro…\n 8    0.001                 5 mn_log_loss binary     0.370     5  0.0152 Prepro…\n 9    0.0000000001          4 mn_log_loss binary     0.380     5  0.0140 Prepro…\n10    0.000000001           4 mn_log_loss binary     0.380     5  0.0140 Prepro…\n# ℹ 90 more rows\n\n\n\n\nBest Tree Model\nUsing select_best(), we can find the tree model with the lowest mn log loss. We then take this model and fit it to the entire training set. After fitting it to the entire training dataset, we fit it to the test dataset to see what the final mn log loss is for this model.\n\ntree_best_params &lt;- select_best(tree_fits, metric= \"mn_log_loss\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                 \n1    0.0000000001          5 Preprocessor1_Model031\n\n#finalize model on training set\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(mn_log_loss))\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.355 Preprocessor1_Model1\n\n\nResults look decent at best. Let’s go ahead and look at the picture of this tree:\n\ntree_final_fit |&gt;\n  extract_workflow() |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint = FALSE, type = 2, extra = 104)\n\n\n\n\nLooks pretty complex. Alas, we still have one more model to test."
  },
  {
    "objectID": "Modeling.html#random-forests",
    "href": "Modeling.html#random-forests",
    "title": "Modeling",
    "section": "Random Forests",
    "text": "Random Forests\nWith random forests, we will be using the same setup\n\nrf_spec &lt;- rand_forest(\n  mtry = tune(),\n  min_n = 10,\n  trees = 500\n) |&gt;\n set_engine(\"ranger\", importance = \"impurity\") |&gt;\n set_mode(\"classification\")\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(tree_rec) |&gt;\n add_model(rf_spec)\n\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 13)),\n  levels = 5\n)\n\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(\n    resamples = cv_folds,\n    grid = rf_grid,\n    metrics = metric_set(mn_log_loss)\n  )\n\nWarning: package 'ranger' was built under R version 4.3.3\n\n\n\nrf_fit |&gt;\n collect_metrics() |&gt;\n filter(.metric == \"mn_log_loss\") |&gt;\n arrange(mean)\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     5 mn_log_loss binary     0.366     5  0.0132 Preprocessor1_Model1\n2     9 mn_log_loss binary     0.378     5  0.0133 Preprocessor1_Model3\n3     7 mn_log_loss binary     0.381     5  0.0143 Preprocessor1_Model2\n4    11 mn_log_loss binary     0.396     5  0.0209 Preprocessor1_Model4\n5    13 mn_log_loss binary     0.396     5  0.0223 Preprocessor1_Model5\n\n\n\nrf_best_params &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     5 Preprocessor1_Model1\n\n\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n finalize_workflow(rf_best_params)\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(data_split, metrics = metric_set(mn_log_loss))\n\nrf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.344 Preprocessor1_Model1\n\n\n\nrf_full_fit &lt;- rf_final_wkf |&gt;\n fit(diabetes_final)\n\nrf_final_model &lt;- extract_fit_engine(rf_full_fit)\n\nrf_imp &lt;- tibble(term = names(importance(rf_final_model)),value = importance(rf_final_model)) |&gt; arrange(desc(value))\nrf_imp\n\n# A tibble: 29 × 2\n   term                    value\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 GenHlth_1                55.6\n 2 HighBP_X1                44.7\n 3 DiffWalk_X1              34.2\n 4 HighChol_X1              31.6\n 5 MentHlth_1               26.0\n 6 GenHlth_3                25.5\n 7 HeartDiseaseorAttack_X1  25.5\n 8 Smoker_X1                24.8\n 9 PhysHlth_1               24.8\n10 Sex_Male                 23.1\n# ℹ 19 more rows\n\n\n\nrf_imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip()\n\n\n\n\n\n prediction &lt;- predict(rf_full_fit, diabetes_final, type = \"class\")\n  \n  results &lt;- diabetes_final |&gt;\n    mutate(Predicted = prediction$.pred_class)\n  \n  confusion &lt;- conf_mat(data = results, truth = Diabetes_binary, estimate = Predicted)\n  \n  confusion_df &lt;- as.data.frame(confusion$table)\n  \n  confusion_plot&lt;- autoplot(confusion, type = \"heatmap\")\n  print(confusion_plot)\n\n\n\n\n\n#saveRDS(rf_final_wkf, file = \"rf_final_wkf.rds\")\n#saveRDS(rf_full_fit, file = \"rf_full_fit.rds\")"
  }
]