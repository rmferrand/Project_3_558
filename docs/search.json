[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "library(tidyverse)\ndiabetes &lt;- read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "Introduction",
    "text": "Introduction\n\nbriefly describe the data and variables we have to work with (in particular, the ones i will be prioritizing)\n\n\nAll Variables and Levels\nhttps://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf\nDiabetes_binary: 0 if no diabetes, 1 if diabetes\nHighBP: 0 if not High Blood Pressure, 1 if High Blood Pressure HighChol: 0 if low, 1 if high CholCheck: 0 if not checked in 5 years, 1 if checked in 5 years Smoker: 0 if not smoked 5 packs of cigarettes, 1 if yes Stroke: 0 if no stroke, 1 if yes stroke HeartDiseaseorAttack: 0 if no heart disease/attack, 1 if yes PhysActivity: 0 if no physical activity in last month, 1 if yes Fruits: 0 if no daily fruit, 1 if daily fruit Veggies: 0 if no daily vegetables, 1 if daily vegetables HvyAlcoholConsump: 0 if not heavy drinker, 1 if heavy drinker (&gt;14 drinks men, &gt;7 women) AnyHealthcare: 0 if no healthcare, 1 if healthcare NoDocbcCost: 0 if not cost-restricted for doctors, 1 if yes DiffWalk: 0 if no difficulty walking, 1 if yes Sex: 0 if female, 1 if male\nGenHlth: 1-5 scale of overall health, 1 is excellent 5 is poor MentHlth: 1-30 scale of days with bad mental health. PhysHlth: 1-30 scale of days with bad physical health. Age: Education: Income:\nBMI: numerical variable of bmi\n\nwhat is the purpose of my eda and the goal with modeling"
  },
  {
    "objectID": "EDA.html#exploratory-data-analysis",
    "href": "EDA.html#exploratory-data-analysis",
    "title": "EDA",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nValidating the Data\nIn terms of exploratory data analysis, I always like to take the first step of exploring missing values:\n\n(sum(is.na(diabetes)))\n\n[1] 0\n\n\nGreat! It looks like there are none. I think a natural next step would be to investigate all of the variables individually, and make sure their names are legible and their values are logical.\n\nas_tibble(diabetes, width = Inf)\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nBased on the given variable descriptions above, I believe that the variable names are rather sensical. However, it looks like the data was read in as entirely double and not as factors. Let’s fix that first for the binary variables. Since the binary variables take values 0 for no and 1 for yes, I figure we can just leave them as is. Sex is assigned to female or male respectively.\n\ndiabetes &lt;- diabetes |&gt;\n  mutate(across(c(1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18), as.factor)) |&gt;\n  mutate(Sex = factor(Sex, levels = c(0,1), labels = c(\"Female\", \"Male\")))\nstr(diabetes)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : num  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : num  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num  3 1 8 6 4 8 7 4 1 3 ...\n\n\nSimilarly, we have other factors with multiple levels that need to be named and ordered. The names could matter later in analysis, and the same thing for the ordering.\nFor PhysHlth and MntlHlth, there were a few considerations. Treating this as a numerical variable possibly loses information between 0 days and 1 day. Furthermore, I wanted to account those who are in pain all of the time. That is why I have decided to transform the levels into 0 = “No Pain”, 1 = “Some Pain”, and 2 = “Everyday Pain”.\nFor the other variables, they are assigned with the labels provided in\n\ndiabetes &lt;- diabetes |&gt; \n  mutate(GenHlth = factor(GenHlth, levels = c(1, 2, 3, 4, 5), labels = c(\"Excellent\", \"Very Good\", \"Good\", \"Fair\", \"Poor\"), ordered = TRUE)) |&gt;\n  mutate(MentHlth = case_when(\n    MentHlth == 0 ~ 0,\n    MentHlth &gt;= 1 & MentHlth &lt;= 29 ~ 1,\n    MentHlth == 30 ~ 2\n  )) |&gt;\n  mutate(MentHlth = factor(MentHlth,levels = c(0, 1, 2), labels = c(\"No Pain\", \"Some Pain\", \"Everyday Pain\"),ordered = TRUE)) |&gt;\nmutate(PhysHlth = case_when(\n    PhysHlth == 0 ~ 0,\n    PhysHlth &gt;= 1 & PhysHlth &lt;= 29 ~ 1,\n    PhysHlth == 30 ~ 2\n  )) |&gt;\n  mutate(PhysHlth = factor(PhysHlth,levels = c(0, 1, 2), labels = c(\"No Pain\", \"Pain\", \"Everyday Pain\"),ordered = TRUE)) |&gt;\n  mutate(Age = factor(Age,levels = c(1:13), labels = c(\"18-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"80-99\"),ordered = TRUE)) |&gt;\n  mutate(Education = factor(Education,levels = c(1:6), labels = c(\"NoSchool\",\"Elementary\",\"Some high school\",\"High school graduate\",\"Some college or technical school\",\"College graduate\"),ordered = TRUE)) |&gt;\n  mutate(Income = factor(Income,levels = c(1:8), labels = c(\"&lt;$10,000\",\"$10,000-$14,999\",\"$15,000-$19,999\",\"$20,000-$24,999\",\"$25,000 - $34,999\",\"$35,000 - $49,999\",\"$50,000 - $74,999\",\"&gt;=$75,000\"),ordered = TRUE))\n\nNow, ALL of the data should be correctly formatted and validated!\n\nstr(diabetes)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Ord.factor w/ 5 levels \"Excellent\"&lt;\"Very Good\"&lt;..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : Ord.factor w/ 3 levels \"No Pain\"&lt;\"Some Pain\"&lt;..: 2 1 3 1 2 1 1 1 3 1 ...\n $ PhysHlth            : Ord.factor w/ 3 levels \"No Pain\"&lt;\"Pain\"&lt;..: 2 1 3 1 1 2 2 1 3 1 ...\n $ DiffWalk            : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Ord.factor w/ 13 levels \"18-24\"&lt;\"25-29\"&lt;..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Ord.factor w/ 6 levels \"NoSchool\"&lt;\"Elementary\"&lt;..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Ord.factor w/ 8 levels \"&lt;$10,000\"&lt;\"$10,000-$14,999\"&lt;..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\n\n\nDetermining an Objective\nNow that all of the data is correctly validated, we can determine which variables we are interested in looking at. I tend to like looking at more unexpected, or indirect routes to find relationships I wouldn’t obviously expect. I believe that diet and the byproducts of a poor one will lead to a higher probability of diabetes. I’ll approach this from a socioeconomic standpoint instead, plus a few extra variables that I am curious about.\nVariables of interest: Fruits, Veggies, AnyHealthCare, NoDocbcCost, GenHlth, MentHlth, PhysHlth, Sex, Age, Education, Income\n\ndiabetes_final &lt;- diabetes |&gt;\n  select(c(Diabetes_binary, AnyHealthcare, NoDocbcCost, GenHlth, MentHlth, PhysHlth, Sex, Age, Education, Income))\n\nThis means we have no numeric variables. I figure that, instead of looking at 12 different table() calls, it may be helpful to see barcharts.\n\nsaveRDS(diabetes_final, file = \"diabetes_final.rds\")\n\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "library(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.0     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'scales' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nset.seed(558)\n\ndiabetes_final &lt;- readRDS(file = \"diabetes_final.rds\")\n\ndiabetes_final &lt;- diabetes_final |&gt; slice_sample(n = 5000)\n\n\ndata_split &lt;- initial_split(diabetes_final, prop = 0.70)\ntrain_diabetes&lt;- training(data_split)\ntest_diabetes &lt;- testing(data_split)\n\n\ncv_folds &lt;- vfold_cv(train_diabetes, v = 5)"
  },
  {
    "objectID": "Modeling.html#train-and-test-split",
    "href": "Modeling.html#train-and-test-split",
    "title": "Modeling",
    "section": "",
    "text": "library(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.0     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'scales' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nset.seed(558)\n\ndiabetes_final &lt;- readRDS(file = \"diabetes_final.rds\")\n\ndiabetes_final &lt;- diabetes_final |&gt; slice_sample(n = 5000)\n\n\ndata_split &lt;- initial_split(diabetes_final, prop = 0.70)\ntrain_diabetes&lt;- training(data_split)\ntest_diabetes &lt;- testing(data_split)\n\n\ncv_folds &lt;- vfold_cv(train_diabetes, v = 5)"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling",
    "section": "Classification Tree",
    "text": "Classification Tree\n\ntree_rec &lt;- recipe(Diabetes_binary ~ ., data = train_diabetes) |&gt;\n  step_dummy(AnyHealthcare, NoDocbcCost, GenHlth, MentHlth, PhysHlth, Sex, Age, Education, Income)\ntree_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 9\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: AnyHealthcare, NoDocbcCost, GenHlth, MentHlth, ...\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 500,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\n#tree_grid &lt;- tibble(\n#  cost_complexity = c(0.01, 0.1),\n#  tree_depth = c(5, 10)\n#)\n\ntree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), levels = 5)\n\n#tree_grid &lt;- expand.grid(\n#  cost_complexity = c(0.00001, 0.0001, 0.001, 0.01, 0.1),\n#  tree_depth = c(1, 3, 5, 7, 10)\n#)\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_mod)\n\ntree_fits &lt;- tree_wkf |&gt; \n  tune_grid(resamples = cv_folds,\n            grid = tree_grid,metrics = metric_set(mn_log_loss))\n\ntree_fits |&gt;\n  collect_metrics()\n\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 2    0.0000000178          1 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 3    0.00000316            1 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 4    0.000562              1 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 5    0.1                   1 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 6    0.0000000001          4 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 7    0.0000000178          4 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 8    0.00000316            4 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 9    0.000562              4 mn_log_loss binary     0.415     5  0.0113 Prepro…\n10    0.1                   4 mn_log_loss binary     0.415     5  0.0113 Prepro…\n# ℹ 15 more rows\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 2    0.0000000178          1 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 3    0.00000316            1 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 4    0.000562              1 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 5    0.1                   1 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 6    0.0000000001          4 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 7    0.0000000178          4 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 8    0.00000316            4 mn_log_loss binary     0.415     5  0.0113 Prepro…\n 9    0.000562              4 mn_log_loss binary     0.415     5  0.0113 Prepro…\n10    0.1                   4 mn_log_loss binary     0.415     5  0.0113 Prepro…\n# ℹ 15 more rows\n\ntree_best_params &lt;- select_best(tree_fits, metric= \"mn_log_loss\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          1 Preprocessor1_Model01\n\n#finalize model on training set\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(mn_log_loss))\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.405 Preprocessor1_Model1\n\ntree_final_fit |&gt;\n  extract_workflow() |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint = FALSE, type = 2, extra = 104)\n\n\n\n\n\nrf_spec &lt;- rand_forest(\n  mtry = tune(),\n  min_n = 500,\n  trees = 200\n) |&gt;\n set_engine(\"ranger\", importance = \"impurity\") |&gt;\n set_mode(\"classification\")\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(tree_rec) |&gt;\n add_model(rf_spec)\n\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(5, 13)),\n  levels = 5\n)\n\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(\n    resamples = cv_folds,\n    grid = rf_grid,\n    metrics = metric_set(mn_log_loss)\n  )\n\nWarning: package 'ranger' was built under R version 4.3.3\n\n\n\nrf_fit |&gt;\n collect_metrics() |&gt;\n filter(.metric == \"mn_log_loss\") |&gt;\n arrange(mean)\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1    13 mn_log_loss binary     0.357     5 0.00945 Preprocessor1_Model5\n2    11 mn_log_loss binary     0.358     5 0.00949 Preprocessor1_Model4\n3     9 mn_log_loss binary     0.359     5 0.00973 Preprocessor1_Model3\n4     7 mn_log_loss binary     0.361     5 0.00994 Preprocessor1_Model2\n5     5 mn_log_loss binary     0.363     5 0.00981 Preprocessor1_Model1\n\n\n\nrf_best_params &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1    13 Preprocessor1_Model5\n\n\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n finalize_workflow(rf_best_params)\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(data_split, metrics = metric_set(mn_log_loss))\n\nrf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.351 Preprocessor1_Model1\n\n\n\nrf_full_fit &lt;- rf_final_wkf |&gt;\n fit(diabetes_final)\n\nrf_final_model &lt;- extract_fit_engine(rf_full_fit)\n\nrf_imp &lt;- tibble(term = names(importance(rf_final_model)),value = importance(rf_final_model)) |&gt; arrange(desc(value))\nrf_imp\n\n# A tibble: 35 × 2\n   term        value\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 GenHlth_1   66.4 \n 2 Age_01      21.2 \n 3 GenHlth_3   19.1 \n 4 GenHlth_4   10.9 \n 5 Age_03       6.81\n 6 GenHlth_2    6.03\n 7 Income_1     4.90\n 8 Age_04       2.80\n 9 PhysHlth_1   2.33\n10 Education_1  1.95\n# ℹ 25 more rows\n\n\n\nrf_imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip()"
  }
]